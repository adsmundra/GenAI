{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAmTRPS9aMWHF5gP8dFk7U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adsmundra/GenAI/blob/main/LLM_Hands_on_Workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ AI Hubs by Kubesimplify X Welzin | Chandigarh Edition  \n",
        "**Beginner's Guide to Building RAG Pipelines with LLMs**  \n",
        "*(Hands-On Workshop | 2 Hours | Bring Your Laptop)*  \n",
        "\n",
        "https://konfhub.com/ai-hubs-meetup-chandigarh-edition\n",
        "\n",
        "\n",
        "The best time to start building with AI was yesterday. The next best is NOW!\n",
        "\n",
        "*Let's turn your LLM ideas into reality!*  "
      ],
      "metadata": {
        "id": "tr_lW8O07_nJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1o9cDFldSX33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üåü **Workshop Highlights**\n",
        "- ‚úÖ **No prior AI experience needed** - Perfect for first-timers!  \n",
        "- ‚úÖ **Deploy a local LLM** with Ollama (no GPU/cloud required)  \n",
        "- ‚úÖ **Build a custom Q&A bot** that understands *your* documents  \n",
        "- ‚úÖ **Take home** fully functional code and Colab notebooks  \n",
        "\n",
        "---\n",
        "\n",
        "## üõ†Ô∏è **What You'll Build**  \n",
        "| Project             | Tools Used          | Outcome                                  |\n",
        "|---------------------|---------------------|------------------------------------------|\n",
        "| Local Text Summarizer | Ollama, LangChain  | Summarize PDFs/websites offline          |\n",
        "| Document Q&A Bot     | ChromaDB, Deepseek| Ask questions about custom datasets      |\n",
        "| Hybrid RAG Pipeline  | SentenceTransformers| Combine keyword + vector search          |\n",
        "|||\n",
        "\n",
        "---\n",
        "\n",
        "## üìù **Agenda**  \n",
        "\n",
        "### **Part 1: LLM Fundamentals (30 mins)**  \n",
        "- Why traditional LLMs fail with custom data?  \n",
        "- RAG architecture explained\n",
        "- Live demo: ChatGPT vs. local Mistral-7B comparison  \n",
        "\n",
        "### **Part 2: Hands-On Lab (90 mins)**  \n",
        "1. **Setup**  \n",
        "   - Install Ollama + load Deepseek model\n",
        "\n",
        "2. **Document Processing**  \n",
        "  - Ingest PDFs/websites ‚Üí chunk text ‚Üí create embeddings  \n",
        "\n",
        "3. **RAG Implementation**  \n",
        "  - Vector DB setup with Chroma  \n",
        "  - LangChain orchestration  \n",
        "\n",
        "4. **Deployment**  \n",
        "  - Build Gradio UI for your Q&A system  \n",
        "  - Share your local endpoint via ngrok  \n",
        "\n"
      ],
      "metadata": {
        "id": "GStNWhyGSYU8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõ†Ô∏è **Key Libraries Explained**\n",
        "\n",
        "- **Ollama**  \n",
        "  - *What it does:* Enables running open-source LLMs locally, such as DeepSeek, Mistral, or Llama, without cloud dependencies.\n",
        "  - *Why use it:* Privacy, cost-free inference, and offline capabilities.\n",
        "\n",
        "- **LangChain**  \n",
        "  - *What it does:* Provides tools for chaining LLM workflows, including document loaders, text splitters, and retrieval-augmented generation logic.\n",
        "  - *Why use it:* Simplifies building complex LLM pipelines and integrates with various models and data sources.\n",
        "\n",
        "- **ChromaDB**  \n",
        "  - *What it does:* Lightweight, in-memory vector database for storing and retrieving document embeddings.\n",
        "  - *Why use it:* Fast, easy to use, and perfect for prototyping RAG systems.\n",
        "\n",
        "- **PyMuPDF**  \n",
        "  - *What it does:* Extracts text and metadata from PDF documents.\n",
        "  - *Why use it:* Essential for processing PDF-based datasets.\n",
        "  \n",
        "- **HuggingFaceEmbeddings**  \n",
        "  - *What it does:* Converts text into vector embeddings using pre-trained models (e.g., `all-MiniLM-L6-v2`).\n",
        "  - *Why use it:* Enables semantic search and retrieval by representing text as vectors.\n",
        "\n",
        "- **Gradio**  \n",
        "  - *What it does:* Builds interactive web interfaces for machine learning models with minimal code.\n",
        "  - *Why use it:* Quickly demo and share your LLM pipeline with others, even non-technical users.\n",
        "\n"
      ],
      "metadata": {
        "id": "faCylU6IVnTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What Is Retrieval Augmented Generation, or RAG?\n",
        "\n",
        "Retrieval augmented generation, or RAG, is an architectural approach that can improve the efficacy of large language model (LLM) applications by leveraging custom data. This is done by retrieving data/documents relevant to a question or task and providing them as context for the LLM.\n",
        "\n",
        "RAG has shown success in support chatbots and Q&A systems that need to maintain up-to-date information or access domain-specific knowledge.\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YLrQl5CM7NjQPcfTCrf-sQ.png\" alt=\"Alt text\" width=\"800\"/>\n",
        "\n",
        "\n",
        "https://www.databricks.com/glossary/retrieval-augmented-generation-rag"
      ],
      "metadata": {
        "id": "wScKXnoiYb-U"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z-MzFwwnSte2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install modules\n",
        "\n",
        "!pip install -q -U  langchain \\\n",
        "                    langchain-community \\\n",
        "                    langchain-huggingface \\\n",
        "                    chromadb \\\n",
        "                    gradio \\\n",
        "                    pymupdf \\\n",
        "                    ollama"
      ],
      "metadata": {
        "id": "RoX7-BcwBZbI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Ollama and CUDA drivers\n",
        "\n",
        "import os\n",
        "\n",
        "!nvidia-smi\n",
        "!curl https://ollama.ai/install.sh | sh\n",
        "!echo 'debconf debconf/frontend select Noninteractive' | sudo debconf-set-selections\n",
        "!sudo apt-get update && sudo apt-get install -y cuda-drivers\n",
        "\n",
        "# Set LD_LIBRARY_PATH so the system NVIDIA library\n",
        "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "i5KtlHthB2qh",
        "outputId": "5d153c5c-a279-417d-fc42-53633865f301"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 13281    0 13281    0     0  69518      0 --:--:-- --:--:-- --:--:-- 69900\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "############################################################################################# 100.0%\n",
            ">>> Adding ollama user to render group...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 3,632 B in 1s (2,863 B/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "cuda-drivers is already the newest version (575.57.08-0ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 218 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure Ollama and pull models in your local\n",
        "\n",
        "!nohup ollama serve &\n",
        "!ollama ps\n",
        "!ollama pull deepseek-r1:1.5b # [deepseek-r1:1.5b, deepseek-r1:14b]\n",
        "!ollama list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTarcYiLIwUX",
        "outputId": "2e2a2e3b-0554-456a-bf28-2de1ff86d4be"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "NAME    ID    SIZE    PROCESSOR    UNTIL \n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "NAME                ID              SIZE      MODIFIED               \n",
            "deepseek-r1:1.5b    e0979632db5a    1.1 GB    Less than a second ago    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required modules\n",
        "\n",
        "from typing import List  # For type hinting\n",
        "\n",
        "from langchain_community.document_loaders import PyMuPDFLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain_community.llms import Ollama\n",
        "\n",
        "from transformers import pipeline\n",
        "import ollama\n",
        "import gradio as gr\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "dsZTegUeCceT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a language model\n",
        "\n",
        "EMBEDDING_MODEL = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")  # ['sentence-transformers/all-MiniLM-L6-v2', 'sentence-transformers/all-mpnet-base-v2']\n",
        "LLM_MODEL = 'deepseek-r1:1.5b'  # ['deepseek-r1:1.5b', 'deepseek-r1:7b', 'deepseek-r1:14b']\n",
        "\n",
        "# def get_llm():\n",
        "#     pipe = pipeline(\"text-generation\",\n",
        "#                     model=LLM_MODEL,\n",
        "#                     device=0)  # Use GPU if available\n",
        "\n",
        "#     return HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "uhyzKYRyBf9I"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve relevant documents and generate an answer\n",
        "\n",
        "def rag_chain(question, retriever):\n",
        "    llm = Ollama(model=LLM_MODEL)  # Initialize Ollama LLM with the specified model\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",  # Or other chain types like \"map_reduce\", \"refine\", \"map_rerank\"\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True  # Optional: Return the source documents as well\n",
        "    )\n",
        "    result = qa_chain({\"query\": question})\n",
        "    return result[\"result\"] # Return the generated answer"
      ],
      "metadata": {
        "id": "M-yC-hMmG5wK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_files(files: List[gr.File]) -> List[dict]:  # Type hint the input as a list of Files\n",
        "    \"\"\"Reads the content of multiple uploaded files and returns a list of Documents.\n",
        "    Args:\n",
        "        files (List[gr.File]): A list of uploaded files (PDF, TXT).\n",
        "    Returns:\n",
        "        list: A list of Langchain Document objects.\n",
        "    Raises ValueError: If any file format is unsupported.\n",
        "    \"\"\"\n",
        "    all_documents = []\n",
        "    for file in files:\n",
        "        if file.name.endswith(\".pdf\"):\n",
        "            loader = PyMuPDFLoader(file.name)\n",
        "            documents = loader.load()\n",
        "            all_documents.extend(documents)  # Extend the list with the new documents\n",
        "        elif file.name.endswith(\".txt\"):\n",
        "            loader = TextLoader(file.name) # Use TextLoader for txt files\n",
        "            documents = loader.load()\n",
        "            all_documents.extend(documents)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported file format: {file.name}. Please upload PDF or TXT files.\")\n",
        "    return all_documents"
      ],
      "metadata": {
        "id": "1sEfpgOv7-17"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process the uploaded files and generate a retriever\n",
        "\n",
        "def process_files(files: List[gr.File]): # Type hint the input as a list of Files\n",
        "    \"\"\"Processes multiple uploaded files and returns a retriever.\n",
        "    Args:\n",
        "        files (List[gr.File]): A list of uploaded files.\n",
        "    Returns:\n",
        "        object: A retriever object. Returns None if no files are provided.\n",
        "    \"\"\"\n",
        "    if not files:\n",
        "        return None\n",
        "    try:\n",
        "        documents = read_files(files)  # Read content of all uploaded files\n",
        "    except ValueError as e:\n",
        "        return str(e)  # Return the error message to the Gradio interface\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=chunks, embedding=EMBEDDING_MODEL, persist_directory=\"./chroma_db\"\n",
        "    )\n",
        "    return vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "Yd6ByEuP-IGP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function to handle Gradio interface logic\n",
        "\n",
        "def ask_question(files: List[gr.File], question):\n",
        "    retriever = process_files(files)\n",
        "    if isinstance(retriever, str):\n",
        "        return retriever, \"\"  # Return error message and empty string for answer\n",
        "    if not retriever:\n",
        "        return \"Please upload at least one valid file (PDF or TXT).\", \"\"\n",
        "    if not question.strip():\n",
        "        return \"Please enter a question.\", \"\"\n",
        "    try:\n",
        "        result = rag_chain(question, retriever)\n",
        "        return \"\", result  # Return empty string for error and the result\n",
        "    except Exception as e:  # Catch any other exceptions during RAG\n",
        "        return f\"An error occurred during processing: {e}\", \"\"  # Return error message and empty string"
      ],
      "metadata": {
        "id": "Ka0UBoTX-IC2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio interface setup with improvements\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "        # Document Question Answering with DeepSeek-R1 and Ollama\n",
        "        Upload one or more PDF or text files and ask questions. The DeepSeek-R1 model, powered by Ollama, will extract relevant information to answer your query.\n",
        "        **Important:** Ensure you have the `deepseek-r1:1.5b` model downloaded via `ollama pull deepseek-r1:1.5b` and Ollama is running.\n",
        "    \"\"\")\n",
        "    with gr.Row():\n",
        "        file_upload = gr.Files(label=\"Upload one or more files (PDF or TXT)\")\n",
        "        question_input = gr.Textbox(label=\"Ask a question\", placeholder=\"Type your question here...\")\n",
        "    with gr.Row():\n",
        "        submit_btn = gr.Button(\"Submit\")\n",
        "    with gr.Row():\n",
        "        error_output = gr.Textbox(label=\"Error Messages\", visible=False, lines=3)  # Added lines for better visibility\n",
        "        answer_output = gr.Textbox(label=\"Answer\", lines=10)\n",
        "    submit_btn.click(\n",
        "        ask_question,\n",
        "        inputs=[file_upload, question_input],\n",
        "        outputs=[error_output, answer_output],\n",
        "    )\n",
        "    gr.Markdown(\"\"\"\n",
        "        **Tips:**\n",
        "        * You can upload multiple files at once.\n",
        "        * For large documents, the processing might take some time.\n",
        "        * Check the \"Error Messages\" box if you encounter any issues.\n",
        "    \"\"\")\n",
        "    with gr.Row(): # Put clear buttons in their own row for better layout.\n",
        "        clear_question_btn = gr.Button(\"Clear Question\")\n",
        "        clear_question_btn.click(lambda: \"\", inputs=None, outputs=question_input)\n",
        "        clear_answer_btn = gr.Button(\"Clear Answer\")\n",
        "        clear_answer_btn.click(lambda: \"\", inputs=None, outputs=answer_output)\n",
        "\n",
        "demo.launch(share=True,\n",
        "            show_error=True,\n",
        "            debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "Roo880i9-IBQ",
        "outputId": "7bd6e718-9921-4a7b-8fdf-923a9f5b4041"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://e1de7c7370380eba4f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://e1de7c7370380eba4f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fJPWzfQXUCrL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}